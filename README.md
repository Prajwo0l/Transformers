# Transformer From Scratch â€“ â€œAttention Is All You Needâ€ Implementation

## ğŸ“Œ Overview

This project is a from-scratch PyTorch implementation of the Transformer architecture proposed in the paper â€œAttention Is All You Needâ€ (Vaswani et al., 2017).
The goal of this project is to deeply understand and replicate the original encoderâ€“decoder Transformer model without relying on high-level frameworks such as HuggingFace Transformers.

The implementation includes all core components of the Transformer, including positional encoding, multi-head self-attention, encoder and decoder layers, and feed-forward networks.

## ğŸ§  Key Concepts Implemented

This repository implements the following components from the original paper:

1. Positional Encoding

Sinusoidal positional encoding as described in the paper

Added to token embeddings to inject sequence order information

2. Multi-Head Attention

Linear projections for Query, Key, and Value

Head splitting and parallel attention computation

Scaled dot-product attention

Head concatenation and final linear projection (fc_out)

3. Transformer Encoder

Stacked encoder layers (configurable number of layers)

Self-attention + residual connections + layer normalization

Position-wise feed-forward networks

4. Transformer Decoder

Masked self-attention for autoregressive decoding

Cross-attention (decoder queries, encoder keys/values)

Feed-forward layers with residual connections and normalization

5. Complete Encoderâ€“Decoder Transformer

End-to-end Transformer architecture replicating the original paper

Modular and reusable PyTorch classes

## ğŸ—ï¸ Project Structure
```text
â”œâ”€â”€ FeedForward.py
â”œâ”€â”€ MultiHeadAttention.py
â”œâ”€â”€ PositionalEncoding.py
â”œâ”€â”€ EncoderLayer.py
â”œâ”€â”€ DecoderLayer.py
â”œâ”€â”€ Transformer.py
â”œâ”€â”€ train.py (optional)
â””â”€â”€ README.md


## âš™ï¸ Technologies Used

Python 3.x

PyTorch

NumPy

Math (for sinusoidal positional encoding)

```python

##  ğŸš€ How to Run
Install dependencies
pip install torch

## Example Usage
```python
from Transformer import Transformer
import torch

model = Transformer(
    d_model=512,
    num_heads=8,
    num_encoder_layers=6,
    num_decoder_layers=6,
    d_ff=2048
)

src = torch.randint(0, 1000, (32, 50))   # batch_size=32, seq_len=50
tgt = torch.randint(0, 1000, (32, 50))

out = model(src, tgt)
print(out.shape)


## ğŸ“– Learning Goals of This Project

Understand the mathematical and architectural foundations of Transformers

Implement attention mechanisms manually instead of using high-level libraries

Learn tensor shape manipulation, masking, and multi-head attention internals

Build a reusable Transformer architecture for future research and experiments

## ğŸ“š Reference Paper

Vaswani et al., Attention Is All You Need, NeurIPS 2017
https://arxiv.org/abs/1706.03762

## ğŸ§© Future Improvements

Add training loop for machine translation tasks

Implement learned and rotary positional embeddings

Add visualization for attention weights

Optimize with PyTorch Lightning or Accelerate

Implement GPT-style decoder-only Transformer

Benchmark against HuggingFace Transformer outputs


## â­ Acknowledgements

This project was built for educational purposes to deeply understand the Transformer architecture and its internal workings.
