# Vision Transformer (ViT) from Scratch in PyTorch

<img src="https://img.shields.io/badge/PyTorch-2.0%2B-ee4c2c?logo=pytorch&logoColor=white" alt="PyTorch">  
<img src="https://img.shields.io/badge/Python-3.8%2B-brightgreen" alt="Python">  
<img src="https://img.shields.io/badge/License-MIT-blue" alt="License">

Implementation of the **Vision Transformer (ViT)** model **from scratch** using only PyTorch â€” no high-level libraries like `timm` or `transformers`.  
Tested and trained on **MNIST** for simplicity and quick experimentation.

Based on the paper:  
**"An Image is Worth 16Ã—16 Words: Transformers for Image Recognition at Scale"** (Dosovitskiy et al., 2020)

---

## ðŸ“Œ Project Goals

- Understand how Vision Transformers work internally
- Implement every major component manually
- Keep the code modular, readable and educational
- Train a small ViT that actually learns on MNIST

---

## âœ¨ Features

- Pure PyTorch implementation
- Modular components (Patch Embedding, Multi-Head Attention, Transformer Encoder, MLP Head)
- Learnable positional embeddings + class token
- Ready-to-run Jupyter notebook for training & visualization
- Small model size â†’ fast training even on CPU / free Colab

---

## ðŸ“‚ Project Structure

```text
vit-from-scratch/
â”œâ”€â”€ Classification.py           # MLP head after [CLS] token
â”œâ”€â”€ MultiHeadAttention.py       # Scaled dot-product multi-head attention
â”œâ”€â”€ PatchEmbedding.py           # Image â†’ patches â†’ linear projection
â”œâ”€â”€ PositionalEncoding.py       # (optional) sinusoidal version â€” not used in main model
â”œâ”€â”€ TransformerEncoder.py       # One transformer block (LN â†’ Attention â†’ LN â†’ MLP)
â”œâ”€â”€ VisionTransformer.py        # Full model: patching + cls token + pos embed + NÃ—encoder + head
â”œâ”€â”€ ViT_Experiment.ipynb        # Training script + visualization (MNIST)
â””â”€â”€ README.md